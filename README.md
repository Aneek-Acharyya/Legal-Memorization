## Do LLMs Reason About Law or Simply Remember It?: A Tale of Three Tasks (ACL 2026 — Under Review) <br>
Large language models (LLMs) are increasingly adopted in the legal domain, despite concerns about their tendency to memorize substantial portions of training data. This work presents the first comprehensive investigation of memorization in legal NLP, analyzing its effects on three core tasks—statute prediction, rhetorical role prediction, and judgment prediction—using legal datasets from India and the United Kingdom. Beyond standard performance evaluation, we assess LLMs’ semantic understanding using memorization-sensitive metrics such as Min-K%, Min-K%++, and sliding-window–based measures, enabling a systematic examination of model trustworthiness. Our study addresses three key questions: whether LLMs learn genuine semantic representations or rely on memorization, how such memorization impacts generalization to unseen data, and whether activation-based steering can exploit memorized knowledge to improve performance on novel datasets. Extensive experiments show that activation-based steering can significantly enhance generalization, yielding performance improvements of approximately 9%.
[image_cropped-3.pdf](https://github.com/user-attachments/files/24649895/image_cropped-3.pdf)
